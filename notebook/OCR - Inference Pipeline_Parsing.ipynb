{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4cbed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#PATH_REPO = \"C:\\\\Users\\\\marie\\\\Desktop\\\\trocr_handwritten\\\\\"\n",
    "PATH_REPO = \"C:\\\\Users\\\\beigelman\\\\Documents\\\\GitHub\\\\trocr_handwritten\\\\\"\n",
    "PATH_MODELS = join(PATH_REPO, 'models')\n",
    "PATH_UTILS = join(PATH_REPO, 'trocr_handwritten','utils')\n",
    "sys.path.append(PATH_UTILS)\n",
    "\n",
    "#changer le path_data uniquement\n",
    "PATH_DATA = join(PATH_REPO, 'data','test')\n",
    "PATH_PAGES = join(PATH_DATA, 'pages')\n",
    "PATH_XML = join(PATH_DATA, 'XML')\n",
    "PATH_OUTPUT = join(PATH_DATA, 'OCRized')\n",
    "PATH_LINES = join(PATH_DATA, 'lines')\n",
    "\n",
    "requirements_path = os.path.join(PATH_REPO, 'requirements.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f15251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | findstr torch_same_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf882742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/CyberZHG/torch-same-pad.git\n",
      "  Cloning https://github.com/CyberZHG/torch-same-pad.git to c:\\users\\beigelman\\appdata\\local\\temp\\pip-req-build-0cdpvghl\n",
      "  Resolved https://github.com/CyberZHG/torch-same-pad.git to commit b60965823567046d0c100fa0e29e36cab6c4ae6b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch-same-pad==0.1.0) (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from torch->torch-same-pad==0.1.0) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from jinja2->torch->torch-same-pad==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\beigelman\\appdata\\local\\pypoetry\\cache\\virtualenvs\\trocr-handwritten-gzvf7xef-py3.11\\lib\\site-packages (from sympy->torch->torch-same-pad==0.1.0) (1.3.0)\n",
      "Building wheels for collected packages: torch-same-pad\n",
      "  Building wheel for torch-same-pad (pyproject.toml): started\n",
      "  Building wheel for torch-same-pad (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for torch-same-pad: filename=torch_same_pad-0.1.0-py3-none-any.whl size=7087 sha256=233d25ad0800fb498c7f71b17c5eb73819c5ed0b48a506f00b8fb35ae6fbb01e\n",
      "  Stored in directory: C:\\Users\\beigelman\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ax52pgxd\\wheels\\df\\b8\\97\\e933d8fe5dbf5f74e4b3d6cafc7bf62c34950dddedac727656\n",
      "Successfully built torch-same-pad\n",
      "Installing collected packages: torch-same-pad\n",
      "Successfully installed torch-same-pad-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/CyberZHG/torch-same-pad.git 'C:\\Users\\beigelman\\AppData\\Local\\Temp\\pip-req-build-0cdpvghl'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/CyberZHG/torch-same-pad.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "143e8750-2aac-439b-9fb5-50312ff36dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trocr_handwritten.utils.arunet' from 'C:\\\\Users\\\\beigelman\\\\Documents\\\\GitHub\\\\trocr_handwritten\\\\trocr_handwritten\\\\utils\\\\arunet.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import trocr_handwritten.utils.arunet as arunet\n",
    "import trocr_handwritten.utils.arunet_utils as arunet_utils\n",
    "importlib.reload(arunet_utils)\n",
    "importlib.reload(arunet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9830fd-0c1a-4cf5-992e-d74e8509bbcd",
   "metadata": {},
   "source": [
    "### Launch Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f79c70f5-19a7-4965-b3a3-b8dfc4be6709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beigelman\\Documents\\GitHub\\trocr_handwritten\\\n",
      "ok\n",
      "2024-10-23 18:01:43,169 - INFO - Creating ARU net...\n",
      "Using ARU-Net\n",
      "2024-10-23 18:01:43,202 - INFO - Getting test loaders...\n",
      "2024-10-23 18:01:43,203 - INFO - Loading checkpoint...\n",
      "=> Loading checkpoint\n",
      "2024-10-23 18:01:43,236 - INFO - Saving test predictions as XML...\n",
      "0%|          | 0/4 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[Ac:\\Users\\beigelman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\trocr-handwritten-GzVf7XEF-py3.11\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "warnings.warn(\n",
      "c:\\Users\\beigelman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\trocr-handwritten-GzVf7XEF-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ..\\aten\\src\\ATen\\native\\Convolution.cpp:1009.)\n",
      "return F.conv2d(input, weight, bias, self.stride,\n",
      "C:\\Users\\beigelman\\Documents\\GitHub\\trocr_handwritten\\trocr_handwritten\\utils\\arunet_utils.py:438: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "up_offset.type != \"LineString\"\n",
      "C:\\Users\\beigelman\\Documents\\GitHub\\trocr_handwritten\\trocr_handwritten\\utils\\arunet_utils.py:440: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "or bot_offset.type != \"LineString\"\n",
      "\n",
      " 25%|██▌       | 1/4 [00:06<00:18,  6.23s/it]\n",
      " 50%|█████     | 2/4 [00:10<00:09,  4.79s/it]\n",
      " 75%|███████▌  | 3/4 [00:13<00:04,  4.27s/it]\n",
      "100%|██████████| 4/4 [00:17<00:00,  4.50s/it]\n",
      "4it [00:17,  4.50s/it]\n",
      "2024-10-23 18:02:01,232 - INFO - Processing page FRANOM53_2DPPC044_0004...\n",
      "2024-10-23 18:02:01,310 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:01,326 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:01,342 - INFO - Displaying image with bboxes...\n",
      "Figure(640x480)\n",
      "2024-10-23 18:02:02,335 - INFO - Processing page FRANOM53_2DPPC044_0005...\n",
      "2024-10-23 18:02:02,424 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,436 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,485 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,485 - INFO - Displaying image with bboxes...\n",
      "Figure(640x480)\n",
      "2024-10-23 18:02:02,654 - INFO - Processing page FRANOM53_2DPPC044_0006...\n",
      "2024-10-23 18:02:02,727 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,736 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,752 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,786 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:02,786 - INFO - Displaying image with bboxes...\n",
      "Figure(640x480)\n",
      "2024-10-23 18:02:02,937 - INFO - Processing page FRANOM53_2DPPC044_0007...\n",
      "2024-10-23 18:02:03,036 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:03,052 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:03,096 - INFO - Saving bbox images...\n",
      "2024-10-23 18:02:03,103 - INFO - Displaying image with bboxes...\n",
      "Figure(640x480)\n",
      "Command executed successfully\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import importlib.util\n",
    "PATH_PARSE = join(PATH_REPO, 'trocr_handwritten', 'parse')\n",
    "print(PATH_REPO)\n",
    "\n",
    "# Add PATH_UTILS to sys.path\n",
    "sys.path.append(PATH_PARSE)\n",
    "\n",
    "# Define the command as a string\n",
    "command = f\"python {join(PATH_PARSE, 'parse_page.py')} --PATH_PAGES {PATH_PAGES} --PATH_MODELS {PATH_MODELS} --PATH_XML {PATH_XML} --PATH_LINES {PATH_LINES} --verbose TRUE\"\n",
    "# Execute the command and capture the output\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "print(\"ok\")\n",
    "# Print the output line by line\n",
    "for line in process.stdout:\n",
    "    print(line.decode('utf-8', errors='ignore').strip())\n",
    "\n",
    "# Wait for the process to finish and get the return code\n",
    "process.wait()\n",
    "return_code = process.returncode\n",
    "\n",
    "# Check if the command executed successfully\n",
    "if return_code == 0:\n",
    "    print(\"Command executed successfully\")\n",
    "else:\n",
    "    print(f\"Command failed with return code {return_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6549ac",
   "metadata": {},
   "source": [
    "### Launch OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "719fa049",
   "metadata": {},
   "source": [
    "Get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0dfc36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "PATH_PARSING = join(PATH_REPO, 'trocr_handwritten', 'parse')\n",
    "PATH_OCRIZING = join(PATH_REPO, 'trocr_handwritten', 'trocr')\n",
    "PATH_MODELS = join(PATH_REPO, 'models')\n",
    "\n",
    "with open(join(PATH_PARSING, \"config.json\")) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "125ff9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trocr_handwritten.utils.arunet_utils' from 'C:\\\\Users\\\\beigelman\\\\Documents\\\\GitHub\\\\trocr_handwritten\\\\trocr_handwritten\\\\utils\\\\arunet_utils.py'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import trocr_handwritten.utils.arunet as arunet\n",
    "import trocr_handwritten.utils.arunet_utils as arunet_utils\n",
    "importlib.reload(arunet)\n",
    "importlib.reload(arunet_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "231b76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARU-Net\n",
      "<class 'trocr_handwritten.utils.arunet.ARUNET'>\n"
     ]
    }
   ],
   "source": [
    "from trocr_handwritten.utils.arunet_utils import create_aru_net\n",
    "\n",
    "model_kwargs = dict(\n",
    "    scale_space_num=config.get(\"SCALE_SPACE_NUM\", 6),\n",
    "    res_depth=config.get(\"RES_DEPTH\", 3),\n",
    "    feat_root=config.get(\"FEAT_ROOT\", 8),\n",
    "    filter_size=config.get(\"FILTER_SIZE\", 3),\n",
    "    pool_size=config.get(\"POOL_SIZE\", 2),\n",
    "    activation_name=config.get(\"ACTIVATION_NAME\", \"relu\"),\n",
    "    model=config.get(\"MODEL\", \"aru\"),\n",
    "    num_scales=config.get(\"NUM_SCALES\", 5),\n",
    ")\n",
    "\n",
    "model = create_aru_net(in_channels=1, out_channels=1, model_kwargs=model_kwargs)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07d02926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARU-Net\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = create_aru_net(in_channels=1, out_channels=1, model_kwargs=model_kwargs).to(device)\n",
    "# Load the checkpoint on the selected device\n",
    "checkpoint = torch.load(join(PATH_MODELS, \"cbad_2019.tar\"), map_location=device)\n",
    "load_checkpoint(checkpoint, model)\n",
    "\n",
    "\n",
    "test_loader = get_test_loaders(\n",
    "        PATH_PAGES,\n",
    "        config.get(\"IMAGE_HEIGHT\", 1024),\n",
    "        config.get(\"IMAGE_WIDTH\", 1024),\n",
    "        config.get(\"PADDING\", True),\n",
    "        config.get(\"NUM_WORKERS\"),\n",
    "        config.get(\"PIN_MEMORY\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d7171",
   "metadata": {},
   "source": [
    "## Parse the XMLs and apply OCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77c11b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "576dfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = {\n",
    "        x.split(\".\")[0]: {} for x in listdir(PATH_PAGES) if \"jpg\" in x.lower()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a75396f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FRANOM53_2DPPC044_0004': {},\n",
       " 'FRANOM53_2DPPC044_0005': {},\n",
       " 'FRANOM53_2DPPC044_0006': {},\n",
       " 'FRANOM53_2DPPC044_0007': {}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5b5bd",
   "metadata": {},
   "source": [
    "## Parsing the XMLS\n",
    "\n",
    "Why do we need to parse the XMLs?\n",
    "\n",
    "After reviewing the OCR - Inference Pipeline_ApplyOCR.ipynb and OCR - Inference Pipeline_AG.ipynb notebooks, it appears that parsing the XML files before applying OCR is not strictly necessary for the OCR process itself. The OCR can be applied directly to the image files.\n",
    "\n",
    "However, parsing the XML files may serve other purposes:\n",
    "\n",
    "1. Extracting metadata or structural information about the documents.\n",
    "2. Identifying specific regions of interest within the images.\n",
    "3. Providing ground truth data for evaluation or training purposes.\n",
    "\n",
    "In the current pipeline, it seems the XML parsing step could be optional or used for supplementary information. The OCR process itself (using the TrOCR model) operates directly on the image data.\n",
    "\n",
    "If the XML files contain important information for post-processing or analysis of the OCR results, then parsing them beforehand could be beneficial. Otherwise, you may choose to skip this step and proceed directly to applying OCR on the images.\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Add the XML parsing step to the pipeline\n",
    "- Add the option to skip the XML parsing step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b487cb",
   "metadata": {},
   "source": [
    "## Apply OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e228b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\beigelman\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24ed8cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|██████████| 224/224 [00:00<?, ?B/s] \n",
      "c:\\Users\\beigelman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\trocr-handwritten-GzVf7XEF-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\beigelman\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer_config.json: 100%|██████████| 1.12k/1.12k [00:00<?, ?B/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.31MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|██████████| 772/772 [00:00<?, ?B/s] \n",
      "config.json: 100%|██████████| 4.91k/4.91k [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:32<00:00, 40.9MB/s]\n",
      "generation_config.json: 100%|██████████| 273/273 [00:00<?, ?B/s] \n",
      "tokenizer_config.json: 100%|██████████| 1.34k/1.34k [00:00<?, ?B/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.14MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.62MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 19.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 957/957 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "trocr_model = 'agomberto/trocr-base-handwritten-fr'\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(trocr_model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ace5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348b34b1",
   "metadata": {},
   "source": [
    "### Line by line approach:\n",
    "\n",
    "- Processes images file by file.\n",
    "- Loads one image at a time.\n",
    "- Applies OCR to each image individually.\n",
    "- Writes results to a text file for each image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de6f8dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 19:12:51,427 - INFO - Loading model and processor...\n",
      "preprocessor_config.json: 100%|██████████| 224/224 [00:00<00:00, 224kB/s]\n",
      "c:\\Users\\beigelman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\trocr-handwritten-GzVf7XEF-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\beigelman\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "warnings.warn(message)\n",
      "tokenizer_config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 1.55MB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.43MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.60MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 772/772 [00:00<?, ?B/s]]\n",
      "Processing folders:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the output line by line\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m process\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Wait for the process to finish and get the return code\u001b[39;00m\n\u001b[0;32m     23\u001b[0m process\u001b[38;5;241m.\u001b[39mwait()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Apply OCR\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from os.path import join\n",
    "\n",
    "PATH_OCRIZING = join(PATH_REPO, 'trocr_handwritten', 'trocr')\n",
    "\n",
    "# Add PATH_OCRIZING to sys.path\n",
    "sys.path.append(PATH_OCRIZING)\n",
    "\n",
    "# Define the command as a string\n",
    "command = f\"python {join(PATH_OCRIZING, 'apply_trocr.py')} --PATH_DATA {PATH_LINES} --PATH_OUTPUT {PATH_OUTPUT} --trocr_model agomberto/trocr-base-handwritten-fr --processor microsoft/trocr-large-handwritten --batch_size 32\"\n",
    "\n",
    "# Execute the command and capture the output\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "# Print the output line by line\n",
    "for line in process.stdout:\n",
    "    print(line.decode('utf-8', errors='ignore').strip())\n",
    "\n",
    "# Wait for the process to finish and get the return code\n",
    "process.wait()\n",
    "return_code = process.returncode\n",
    "\n",
    "# Check if the command executed successfully\n",
    "if return_code == 0:\n",
    "    print(\"OCR process completed successfully\")\n",
    "else:\n",
    "    print(f\"OCR process failed with return code {return_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518a5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "344a4a16",
   "metadata": {},
   "source": [
    "### Batch processing approach: \n",
    "\n",
    "- Processes images folder by folder.\n",
    "- Loads all images in a folder into memory at once.\n",
    "- Applies OCR to all images in a folder in a single batch.\n",
    "- Writes results to a text file for each folder.\n",
    "\n",
    "Comparison with your running apply_ocr line by line:\n",
    "- Higher memory usage as it loads all images in a folder at once.\n",
    "- Potentially faster for GPU processing due to batch processing.\n",
    "- Processes and reports results by folder.\n",
    "- f an error occurs, it might affect the entire folder's processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2d9ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['column_1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_1\n",
      "['FRANOM53_2DPPC044_0004_line_0.jpg', 'FRANOM53_2DPPC044_0004_line_1.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Ile coussigné, Déréguer de l'Hégital maritime de ce étatime de trente\", 'le sieur Nicolas, Julien) soldat de la Communeagnic du Sr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "folders = [x for x in listdir(PATH_LINES) if \".\" not in x]\n",
    "folders=['column_1']\n",
    "print(folders)\n",
    "for folder in tqdm(folders):\n",
    "    print(folder)\n",
    "    images_files = [\n",
    "        x\n",
    "        for x in listdir(join(PATH_LINES, folder))\n",
    "        if \".jpg\" in x\n",
    "    ]\n",
    "    print(images_files)\n",
    "    images_files.sort()\n",
    "\n",
    "    bboxes = [[float(y) for y in x[:-4].split('_line_')[1].split('_')] for x in images_files]\n",
    "\n",
    "    images = [Image.open(join(PATH_LINES, folder, img)) for img in images_files]\n",
    "    \n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_texts = tokenizer.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(generated_texts)\n",
    "    os.makedirs(join(PATH_OUTPUT, folder), exist_ok=True)\n",
    "    with open(join(PATH_OUTPUT, folder, \"ocrized.txt\"), \"w\") as f:\n",
    "        for image, bbox, generated_text in zip(images_files, bboxes, generated_texts):\n",
    "            # Convert bbox list to a string\n",
    "            bbox_str = \"_\".join(map(str, bbox))\n",
    "            # Write generated_text, image filename, and bbox to the file\n",
    "            f.write(f\"{generated_text}\\t{image}\\t{bbox_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3664a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
